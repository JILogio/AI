{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract class Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutLayer(Layer):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.mask = np.random.binomial(1, 1 - self.p, size=input_data.shape)\n",
    "        self.output = input_data * self.mask\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return output_error * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "        for i in range(samples):\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        samples = len(x_train)\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "                err += self.loss(y_train[j], output)\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the XOR problem without Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/500   error=0.698956\n",
      "epoch 2/500   error=0.474182\n",
      "epoch 3/500   error=0.348177\n",
      "epoch 4/500   error=0.323970\n",
      "epoch 5/500   error=0.317388\n",
      "epoch 6/500   error=0.314884\n",
      "epoch 7/500   error=0.313556\n",
      "epoch 8/500   error=0.312545\n",
      "epoch 9/500   error=0.311595\n",
      "epoch 10/500   error=0.310635\n",
      "epoch 11/500   error=0.309654\n",
      "epoch 12/500   error=0.308653\n",
      "epoch 13/500   error=0.307637\n",
      "epoch 14/500   error=0.306609\n",
      "epoch 15/500   error=0.305570\n",
      "epoch 16/500   error=0.304522\n",
      "epoch 17/500   error=0.303462\n",
      "epoch 18/500   error=0.302391\n",
      "epoch 19/500   error=0.301304\n",
      "epoch 20/500   error=0.300200\n",
      "epoch 21/500   error=0.299075\n",
      "epoch 22/500   error=0.297927\n",
      "epoch 23/500   error=0.296751\n",
      "epoch 24/500   error=0.295544\n",
      "epoch 25/500   error=0.294302\n",
      "epoch 26/500   error=0.293021\n",
      "epoch 27/500   error=0.291699\n",
      "epoch 28/500   error=0.290329\n",
      "epoch 29/500   error=0.288910\n",
      "epoch 30/500   error=0.287437\n",
      "epoch 31/500   error=0.285906\n",
      "epoch 32/500   error=0.284314\n",
      "epoch 33/500   error=0.282658\n",
      "epoch 34/500   error=0.280933\n",
      "epoch 35/500   error=0.279138\n",
      "epoch 36/500   error=0.277269\n",
      "epoch 37/500   error=0.275324\n",
      "epoch 38/500   error=0.273301\n",
      "epoch 39/500   error=0.271199\n",
      "epoch 40/500   error=0.269017\n",
      "epoch 41/500   error=0.266755\n",
      "epoch 42/500   error=0.264413\n",
      "epoch 43/500   error=0.261993\n",
      "epoch 44/500   error=0.259497\n",
      "epoch 45/500   error=0.256926\n",
      "epoch 46/500   error=0.254286\n",
      "epoch 47/500   error=0.251581\n",
      "epoch 48/500   error=0.248814\n",
      "epoch 49/500   error=0.245992\n",
      "epoch 50/500   error=0.243121\n",
      "epoch 51/500   error=0.240206\n",
      "epoch 52/500   error=0.237252\n",
      "epoch 53/500   error=0.234266\n",
      "epoch 54/500   error=0.231252\n",
      "epoch 55/500   error=0.228214\n",
      "epoch 56/500   error=0.225155\n",
      "epoch 57/500   error=0.222076\n",
      "epoch 58/500   error=0.218977\n",
      "epoch 59/500   error=0.215857\n",
      "epoch 60/500   error=0.212713\n",
      "epoch 61/500   error=0.209541\n",
      "epoch 62/500   error=0.206333\n",
      "epoch 63/500   error=0.203083\n",
      "epoch 64/500   error=0.199780\n",
      "epoch 65/500   error=0.196415\n",
      "epoch 66/500   error=0.192974\n",
      "epoch 67/500   error=0.189446\n",
      "epoch 68/500   error=0.185815\n",
      "epoch 69/500   error=0.182067\n",
      "epoch 70/500   error=0.178187\n",
      "epoch 71/500   error=0.174157\n",
      "epoch 72/500   error=0.169962\n",
      "epoch 73/500   error=0.165587\n",
      "epoch 74/500   error=0.161018\n",
      "epoch 75/500   error=0.156241\n",
      "epoch 76/500   error=0.151247\n",
      "epoch 77/500   error=0.146031\n",
      "epoch 78/500   error=0.140591\n",
      "epoch 79/500   error=0.134933\n",
      "epoch 80/500   error=0.129072\n",
      "epoch 81/500   error=0.123029\n",
      "epoch 82/500   error=0.116838\n",
      "epoch 83/500   error=0.110541\n",
      "epoch 84/500   error=0.104190\n",
      "epoch 85/500   error=0.097846\n",
      "epoch 86/500   error=0.091571\n",
      "epoch 87/500   error=0.085433\n",
      "epoch 88/500   error=0.079493\n",
      "epoch 89/500   error=0.073809\n",
      "epoch 90/500   error=0.068425\n",
      "epoch 91/500   error=0.063375\n",
      "epoch 92/500   error=0.058681\n",
      "epoch 93/500   error=0.054350\n",
      "epoch 94/500   error=0.050378\n",
      "epoch 95/500   error=0.046754\n",
      "epoch 96/500   error=0.043457\n",
      "epoch 97/500   error=0.040465\n",
      "epoch 98/500   error=0.037753\n",
      "epoch 99/500   error=0.035295\n",
      "epoch 100/500   error=0.033066\n",
      "epoch 101/500   error=0.031043\n",
      "epoch 102/500   error=0.029204\n",
      "epoch 103/500   error=0.027530\n",
      "epoch 104/500   error=0.026003\n",
      "epoch 105/500   error=0.024607\n",
      "epoch 106/500   error=0.023329\n",
      "epoch 107/500   error=0.022155\n",
      "epoch 108/500   error=0.021076\n",
      "epoch 109/500   error=0.020081\n",
      "epoch 110/500   error=0.019163\n",
      "epoch 111/500   error=0.018313\n",
      "epoch 112/500   error=0.017524\n",
      "epoch 113/500   error=0.016792\n",
      "epoch 114/500   error=0.016110\n",
      "epoch 115/500   error=0.015475\n",
      "epoch 116/500   error=0.014881\n",
      "epoch 117/500   error=0.014325\n",
      "epoch 118/500   error=0.013805\n",
      "epoch 119/500   error=0.013316\n",
      "epoch 120/500   error=0.012857\n",
      "epoch 121/500   error=0.012425\n",
      "epoch 122/500   error=0.012017\n",
      "epoch 123/500   error=0.011633\n",
      "epoch 124/500   error=0.011269\n",
      "epoch 125/500   error=0.010925\n",
      "epoch 126/500   error=0.010599\n",
      "epoch 127/500   error=0.010290\n",
      "epoch 128/500   error=0.009997\n",
      "epoch 129/500   error=0.009718\n",
      "epoch 130/500   error=0.009453\n",
      "epoch 131/500   error=0.009200\n",
      "epoch 132/500   error=0.008959\n",
      "epoch 133/500   error=0.008729\n",
      "epoch 134/500   error=0.008510\n",
      "epoch 135/500   error=0.008300\n",
      "epoch 136/500   error=0.008099\n",
      "epoch 137/500   error=0.007907\n",
      "epoch 138/500   error=0.007723\n",
      "epoch 139/500   error=0.007546\n",
      "epoch 140/500   error=0.007376\n",
      "epoch 141/500   error=0.007214\n",
      "epoch 142/500   error=0.007057\n",
      "epoch 143/500   error=0.006907\n",
      "epoch 144/500   error=0.006762\n",
      "epoch 145/500   error=0.006623\n",
      "epoch 146/500   error=0.006489\n",
      "epoch 147/500   error=0.006359\n",
      "epoch 148/500   error=0.006235\n",
      "epoch 149/500   error=0.006114\n",
      "epoch 150/500   error=0.005998\n",
      "epoch 151/500   error=0.005886\n",
      "epoch 152/500   error=0.005777\n",
      "epoch 153/500   error=0.005672\n",
      "epoch 154/500   error=0.005571\n",
      "epoch 155/500   error=0.005472\n",
      "epoch 156/500   error=0.005377\n",
      "epoch 157/500   error=0.005285\n",
      "epoch 158/500   error=0.005196\n",
      "epoch 159/500   error=0.005109\n",
      "epoch 160/500   error=0.005025\n",
      "epoch 161/500   error=0.004943\n",
      "epoch 162/500   error=0.004864\n",
      "epoch 163/500   error=0.004787\n",
      "epoch 164/500   error=0.004713\n",
      "epoch 165/500   error=0.004640\n",
      "epoch 166/500   error=0.004570\n",
      "epoch 167/500   error=0.004501\n",
      "epoch 168/500   error=0.004434\n",
      "epoch 169/500   error=0.004369\n",
      "epoch 170/500   error=0.004306\n",
      "epoch 171/500   error=0.004245\n",
      "epoch 172/500   error=0.004185\n",
      "epoch 173/500   error=0.004126\n",
      "epoch 174/500   error=0.004069\n",
      "epoch 175/500   error=0.004014\n",
      "epoch 176/500   error=0.003959\n",
      "epoch 177/500   error=0.003907\n",
      "epoch 178/500   error=0.003855\n",
      "epoch 179/500   error=0.003805\n",
      "epoch 180/500   error=0.003756\n",
      "epoch 181/500   error=0.003708\n",
      "epoch 182/500   error=0.003661\n",
      "epoch 183/500   error=0.003615\n",
      "epoch 184/500   error=0.003570\n",
      "epoch 185/500   error=0.003527\n",
      "epoch 186/500   error=0.003484\n",
      "epoch 187/500   error=0.003442\n",
      "epoch 188/500   error=0.003401\n",
      "epoch 189/500   error=0.003361\n",
      "epoch 190/500   error=0.003322\n",
      "epoch 191/500   error=0.003284\n",
      "epoch 192/500   error=0.003246\n",
      "epoch 193/500   error=0.003210\n",
      "epoch 194/500   error=0.003174\n",
      "epoch 195/500   error=0.003138\n",
      "epoch 196/500   error=0.003104\n",
      "epoch 197/500   error=0.003070\n",
      "epoch 198/500   error=0.003037\n",
      "epoch 199/500   error=0.003005\n",
      "epoch 200/500   error=0.002973\n",
      "epoch 201/500   error=0.002942\n",
      "epoch 202/500   error=0.002911\n",
      "epoch 203/500   error=0.002881\n",
      "epoch 204/500   error=0.002851\n",
      "epoch 205/500   error=0.002822\n",
      "epoch 206/500   error=0.002794\n",
      "epoch 207/500   error=0.002766\n",
      "epoch 208/500   error=0.002739\n",
      "epoch 209/500   error=0.002712\n",
      "epoch 210/500   error=0.002686\n",
      "epoch 211/500   error=0.002660\n",
      "epoch 212/500   error=0.002634\n",
      "epoch 213/500   error=0.002609\n",
      "epoch 214/500   error=0.002585\n",
      "epoch 215/500   error=0.002561\n",
      "epoch 216/500   error=0.002537\n",
      "epoch 217/500   error=0.002514\n",
      "epoch 218/500   error=0.002491\n",
      "epoch 219/500   error=0.002468\n",
      "epoch 220/500   error=0.002446\n",
      "epoch 221/500   error=0.002424\n",
      "epoch 222/500   error=0.002403\n",
      "epoch 223/500   error=0.002382\n",
      "epoch 224/500   error=0.002361\n",
      "epoch 225/500   error=0.002341\n",
      "epoch 226/500   error=0.002321\n",
      "epoch 227/500   error=0.002301\n",
      "epoch 228/500   error=0.002282\n",
      "epoch 229/500   error=0.002262\n",
      "epoch 230/500   error=0.002244\n",
      "epoch 231/500   error=0.002225\n",
      "epoch 232/500   error=0.002207\n",
      "epoch 233/500   error=0.002189\n",
      "epoch 234/500   error=0.002171\n",
      "epoch 235/500   error=0.002154\n",
      "epoch 236/500   error=0.002136\n",
      "epoch 237/500   error=0.002119\n",
      "epoch 238/500   error=0.002103\n",
      "epoch 239/500   error=0.002086\n",
      "epoch 240/500   error=0.002070\n",
      "epoch 241/500   error=0.002054\n",
      "epoch 242/500   error=0.002038\n",
      "epoch 243/500   error=0.002023\n",
      "epoch 244/500   error=0.002007\n",
      "epoch 245/500   error=0.001992\n",
      "epoch 246/500   error=0.001977\n",
      "epoch 247/500   error=0.001963\n",
      "epoch 248/500   error=0.001948\n",
      "epoch 249/500   error=0.001934\n",
      "epoch 250/500   error=0.001920\n",
      "epoch 251/500   error=0.001906\n",
      "epoch 252/500   error=0.001892\n",
      "epoch 253/500   error=0.001879\n",
      "epoch 254/500   error=0.001865\n",
      "epoch 255/500   error=0.001852\n",
      "epoch 256/500   error=0.001839\n",
      "epoch 257/500   error=0.001826\n",
      "epoch 258/500   error=0.001814\n",
      "epoch 259/500   error=0.001801\n",
      "epoch 260/500   error=0.001789\n",
      "epoch 261/500   error=0.001777\n",
      "epoch 262/500   error=0.001765\n",
      "epoch 263/500   error=0.001753\n",
      "epoch 264/500   error=0.001741\n",
      "epoch 265/500   error=0.001730\n",
      "epoch 266/500   error=0.001718\n",
      "epoch 267/500   error=0.001707\n",
      "epoch 268/500   error=0.001696\n",
      "epoch 269/500   error=0.001685\n",
      "epoch 270/500   error=0.001674\n",
      "epoch 271/500   error=0.001663\n",
      "epoch 272/500   error=0.001652\n",
      "epoch 273/500   error=0.001642\n",
      "epoch 274/500   error=0.001631\n",
      "epoch 275/500   error=0.001621\n",
      "epoch 276/500   error=0.001611\n",
      "epoch 277/500   error=0.001601\n",
      "epoch 278/500   error=0.001591\n",
      "epoch 279/500   error=0.001581\n",
      "epoch 280/500   error=0.001572\n",
      "epoch 281/500   error=0.001562\n",
      "epoch 282/500   error=0.001553\n",
      "epoch 283/500   error=0.001543\n",
      "epoch 284/500   error=0.001534\n",
      "epoch 285/500   error=0.001525\n",
      "epoch 286/500   error=0.001516\n",
      "epoch 287/500   error=0.001507\n",
      "epoch 288/500   error=0.001498\n",
      "epoch 289/500   error=0.001489\n",
      "epoch 290/500   error=0.001481\n",
      "epoch 291/500   error=0.001472\n",
      "epoch 292/500   error=0.001464\n",
      "epoch 293/500   error=0.001455\n",
      "epoch 294/500   error=0.001447\n",
      "epoch 295/500   error=0.001439\n",
      "epoch 296/500   error=0.001431\n",
      "epoch 297/500   error=0.001423\n",
      "epoch 298/500   error=0.001415\n",
      "epoch 299/500   error=0.001407\n",
      "epoch 300/500   error=0.001399\n",
      "epoch 301/500   error=0.001391\n",
      "epoch 302/500   error=0.001384\n",
      "epoch 303/500   error=0.001376\n",
      "epoch 304/500   error=0.001369\n",
      "epoch 305/500   error=0.001361\n",
      "epoch 306/500   error=0.001354\n",
      "epoch 307/500   error=0.001347\n",
      "epoch 308/500   error=0.001340\n",
      "epoch 309/500   error=0.001333\n",
      "epoch 310/500   error=0.001325\n",
      "epoch 311/500   error=0.001319\n",
      "epoch 312/500   error=0.001312\n",
      "epoch 313/500   error=0.001305\n",
      "epoch 314/500   error=0.001298\n",
      "epoch 315/500   error=0.001291\n",
      "epoch 316/500   error=0.001285\n",
      "epoch 317/500   error=0.001278\n",
      "epoch 318/500   error=0.001272\n",
      "epoch 319/500   error=0.001265\n",
      "epoch 320/500   error=0.001259\n",
      "epoch 321/500   error=0.001253\n",
      "epoch 322/500   error=0.001246\n",
      "epoch 323/500   error=0.001240\n",
      "epoch 324/500   error=0.001234\n",
      "epoch 325/500   error=0.001228\n",
      "epoch 326/500   error=0.001222\n",
      "epoch 327/500   error=0.001216\n",
      "epoch 328/500   error=0.001210\n",
      "epoch 329/500   error=0.001204\n",
      "epoch 330/500   error=0.001198\n",
      "epoch 331/500   error=0.001192\n",
      "epoch 332/500   error=0.001187\n",
      "epoch 333/500   error=0.001181\n",
      "epoch 334/500   error=0.001176\n",
      "epoch 335/500   error=0.001170\n",
      "epoch 336/500   error=0.001164\n",
      "epoch 337/500   error=0.001159\n",
      "epoch 338/500   error=0.001154\n",
      "epoch 339/500   error=0.001148\n",
      "epoch 340/500   error=0.001143\n",
      "epoch 341/500   error=0.001138\n",
      "epoch 342/500   error=0.001132\n",
      "epoch 343/500   error=0.001127\n",
      "epoch 344/500   error=0.001122\n",
      "epoch 345/500   error=0.001117\n",
      "epoch 346/500   error=0.001112\n",
      "epoch 347/500   error=0.001107\n",
      "epoch 348/500   error=0.001102\n",
      "epoch 349/500   error=0.001097\n",
      "epoch 350/500   error=0.001092\n",
      "epoch 351/500   error=0.001087\n",
      "epoch 352/500   error=0.001083\n",
      "epoch 353/500   error=0.001078\n",
      "epoch 354/500   error=0.001073\n",
      "epoch 355/500   error=0.001068\n",
      "epoch 356/500   error=0.001064\n",
      "epoch 357/500   error=0.001059\n",
      "epoch 358/500   error=0.001055\n",
      "epoch 359/500   error=0.001050\n",
      "epoch 360/500   error=0.001046\n",
      "epoch 361/500   error=0.001041\n",
      "epoch 362/500   error=0.001037\n",
      "epoch 363/500   error=0.001032\n",
      "epoch 364/500   error=0.001028\n",
      "epoch 365/500   error=0.001024\n",
      "epoch 366/500   error=0.001019\n",
      "epoch 367/500   error=0.001015\n",
      "epoch 368/500   error=0.001011\n",
      "epoch 369/500   error=0.001007\n",
      "epoch 370/500   error=0.001002\n",
      "epoch 371/500   error=0.000998\n",
      "epoch 372/500   error=0.000994\n",
      "epoch 373/500   error=0.000990\n",
      "epoch 374/500   error=0.000986\n",
      "epoch 375/500   error=0.000982\n",
      "epoch 376/500   error=0.000978\n",
      "epoch 377/500   error=0.000974\n",
      "epoch 378/500   error=0.000970\n",
      "epoch 379/500   error=0.000967\n",
      "epoch 380/500   error=0.000963\n",
      "epoch 381/500   error=0.000959\n",
      "epoch 382/500   error=0.000955\n",
      "epoch 383/500   error=0.000951\n",
      "epoch 384/500   error=0.000948\n",
      "epoch 385/500   error=0.000944\n",
      "epoch 386/500   error=0.000940\n",
      "epoch 387/500   error=0.000937\n",
      "epoch 388/500   error=0.000933\n",
      "epoch 389/500   error=0.000929\n",
      "epoch 390/500   error=0.000926\n",
      "epoch 391/500   error=0.000922\n",
      "epoch 392/500   error=0.000919\n",
      "epoch 393/500   error=0.000915\n",
      "epoch 394/500   error=0.000912\n",
      "epoch 395/500   error=0.000908\n",
      "epoch 396/500   error=0.000905\n",
      "epoch 397/500   error=0.000901\n",
      "epoch 398/500   error=0.000898\n",
      "epoch 399/500   error=0.000895\n",
      "epoch 400/500   error=0.000891\n",
      "epoch 401/500   error=0.000888\n",
      "epoch 402/500   error=0.000885\n",
      "epoch 403/500   error=0.000882\n",
      "epoch 404/500   error=0.000878\n",
      "epoch 405/500   error=0.000875\n",
      "epoch 406/500   error=0.000872\n",
      "epoch 407/500   error=0.000869\n",
      "epoch 408/500   error=0.000866\n",
      "epoch 409/500   error=0.000862\n",
      "epoch 410/500   error=0.000859\n",
      "epoch 411/500   error=0.000856\n",
      "epoch 412/500   error=0.000853\n",
      "epoch 413/500   error=0.000850\n",
      "epoch 414/500   error=0.000847\n",
      "epoch 415/500   error=0.000844\n",
      "epoch 416/500   error=0.000841\n",
      "epoch 417/500   error=0.000838\n",
      "epoch 418/500   error=0.000835\n",
      "epoch 419/500   error=0.000832\n",
      "epoch 420/500   error=0.000829\n",
      "epoch 421/500   error=0.000827\n",
      "epoch 422/500   error=0.000824\n",
      "epoch 423/500   error=0.000821\n",
      "epoch 424/500   error=0.000818\n",
      "epoch 425/500   error=0.000815\n",
      "epoch 426/500   error=0.000812\n",
      "epoch 427/500   error=0.000810\n",
      "epoch 428/500   error=0.000807\n",
      "epoch 429/500   error=0.000804\n",
      "epoch 430/500   error=0.000801\n",
      "epoch 431/500   error=0.000799\n",
      "epoch 432/500   error=0.000796\n",
      "epoch 433/500   error=0.000793\n",
      "epoch 434/500   error=0.000791\n",
      "epoch 435/500   error=0.000788\n",
      "epoch 436/500   error=0.000785\n",
      "epoch 437/500   error=0.000783\n",
      "epoch 438/500   error=0.000780\n",
      "epoch 439/500   error=0.000778\n",
      "epoch 440/500   error=0.000775\n",
      "epoch 441/500   error=0.000773\n",
      "epoch 442/500   error=0.000770\n",
      "epoch 443/500   error=0.000768\n",
      "epoch 444/500   error=0.000765\n",
      "epoch 445/500   error=0.000763\n",
      "epoch 446/500   error=0.000760\n",
      "epoch 447/500   error=0.000758\n",
      "epoch 448/500   error=0.000755\n",
      "epoch 449/500   error=0.000753\n",
      "epoch 450/500   error=0.000750\n",
      "epoch 451/500   error=0.000748\n",
      "epoch 452/500   error=0.000746\n",
      "epoch 453/500   error=0.000743\n",
      "epoch 454/500   error=0.000741\n",
      "epoch 455/500   error=0.000739\n",
      "epoch 456/500   error=0.000736\n",
      "epoch 457/500   error=0.000734\n",
      "epoch 458/500   error=0.000732\n",
      "epoch 459/500   error=0.000729\n",
      "epoch 460/500   error=0.000727\n",
      "epoch 461/500   error=0.000725\n",
      "epoch 462/500   error=0.000723\n",
      "epoch 463/500   error=0.000720\n",
      "epoch 464/500   error=0.000718\n",
      "epoch 465/500   error=0.000716\n",
      "epoch 466/500   error=0.000714\n",
      "epoch 467/500   error=0.000712\n",
      "epoch 468/500   error=0.000710\n",
      "epoch 469/500   error=0.000707\n",
      "epoch 470/500   error=0.000705\n",
      "epoch 471/500   error=0.000703\n",
      "epoch 472/500   error=0.000701\n",
      "epoch 473/500   error=0.000699\n",
      "epoch 474/500   error=0.000697\n",
      "epoch 475/500   error=0.000695\n",
      "epoch 476/500   error=0.000693\n",
      "epoch 477/500   error=0.000691\n",
      "epoch 478/500   error=0.000689\n",
      "epoch 479/500   error=0.000687\n",
      "epoch 480/500   error=0.000685\n",
      "epoch 481/500   error=0.000683\n",
      "epoch 482/500   error=0.000681\n",
      "epoch 483/500   error=0.000679\n",
      "epoch 484/500   error=0.000677\n",
      "epoch 485/500   error=0.000675\n",
      "epoch 486/500   error=0.000673\n",
      "epoch 487/500   error=0.000671\n",
      "epoch 488/500   error=0.000669\n",
      "epoch 489/500   error=0.000667\n",
      "epoch 490/500   error=0.000665\n",
      "epoch 491/500   error=0.000663\n",
      "epoch 492/500   error=0.000661\n",
      "epoch 493/500   error=0.000659\n",
      "epoch 494/500   error=0.000657\n",
      "epoch 495/500   error=0.000656\n",
      "epoch 496/500   error=0.000654\n",
      "epoch 497/500   error=0.000652\n",
      "epoch 498/500   error=0.000650\n",
      "epoch 499/500   error=0.000648\n",
      "epoch 500/500   error=0.000646\n",
      "[array([[0.00113308]]), array([[0.9654789]]), array([[0.962953]]), array([[-0.00174156]])]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "np.random.seed(10)\n",
    "net = Network()\n",
    "net.add(FCLayer(2, 10))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(10, 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "#net.add(DropoutLayer(p=0.1))\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=500, learning_rate=0.1)\n",
    "out = net.predict(x_train)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the XOR problem with Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/500   error=0.698956\n",
      "epoch 2/500   error=0.474182\n",
      "epoch 3/500   error=0.348177\n",
      "epoch 4/500   error=0.411303\n",
      "epoch 5/500   error=0.311721\n",
      "epoch 6/500   error=0.218378\n",
      "epoch 7/500   error=0.310327\n",
      "epoch 8/500   error=0.311594\n",
      "epoch 9/500   error=0.311889\n",
      "epoch 10/500   error=0.311422\n",
      "epoch 11/500   error=0.310600\n",
      "epoch 12/500   error=0.309632\n",
      "epoch 13/500   error=0.308604\n",
      "epoch 14/500   error=0.084907\n",
      "epoch 15/500   error=0.306404\n",
      "epoch 16/500   error=0.302180\n",
      "epoch 17/500   error=0.302471\n",
      "epoch 18/500   error=0.302800\n",
      "epoch 19/500   error=0.302429\n",
      "epoch 20/500   error=0.301619\n",
      "epoch 21/500   error=0.300601\n",
      "epoch 22/500   error=0.299484\n",
      "epoch 23/500   error=0.298312\n",
      "epoch 24/500   error=0.620599\n",
      "epoch 25/500   error=0.430504\n",
      "epoch 26/500   error=0.366088\n",
      "epoch 27/500   error=0.307657\n",
      "epoch 28/500   error=0.298566\n",
      "epoch 29/500   error=0.294816\n",
      "epoch 30/500   error=0.292409\n",
      "epoch 31/500   error=0.290471\n",
      "epoch 32/500   error=0.288711\n",
      "epoch 33/500   error=0.287010\n",
      "epoch 34/500   error=0.285313\n",
      "epoch 35/500   error=0.283589\n",
      "epoch 36/500   error=0.281820\n",
      "epoch 37/500   error=0.279997\n",
      "epoch 38/500   error=0.278110\n",
      "epoch 39/500   error=0.276153\n",
      "epoch 40/500   error=0.274122\n",
      "epoch 41/500   error=0.272014\n",
      "epoch 42/500   error=0.269827\n",
      "epoch 43/500   error=0.139437\n",
      "epoch 44/500   error=0.264229\n",
      "epoch 45/500   error=0.259387\n",
      "epoch 46/500   error=0.209332\n",
      "epoch 47/500   error=0.257005\n",
      "epoch 48/500   error=0.254936\n",
      "epoch 49/500   error=0.252774\n",
      "epoch 50/500   error=0.250283\n",
      "epoch 51/500   error=0.247604\n",
      "epoch 52/500   error=0.116621\n",
      "epoch 53/500   error=0.242580\n",
      "epoch 54/500   error=0.235963\n",
      "epoch 55/500   error=0.443779\n",
      "epoch 56/500   error=0.407616\n",
      "epoch 57/500   error=0.384648\n",
      "epoch 58/500   error=0.380000\n",
      "epoch 59/500   error=0.225654\n",
      "epoch 60/500   error=0.223173\n",
      "epoch 61/500   error=0.393296\n",
      "epoch 62/500   error=0.236601\n",
      "epoch 63/500   error=0.221147\n",
      "epoch 64/500   error=0.216293\n",
      "epoch 65/500   error=0.212481\n",
      "epoch 66/500   error=0.208931\n",
      "epoch 67/500   error=0.205473\n",
      "epoch 68/500   error=0.202039\n",
      "epoch 69/500   error=0.552314\n",
      "epoch 70/500   error=0.232280\n",
      "epoch 71/500   error=0.199154\n",
      "epoch 72/500   error=0.348926\n",
      "epoch 73/500   error=0.214583\n",
      "epoch 74/500   error=0.191973\n",
      "epoch 75/500   error=0.184691\n",
      "epoch 76/500   error=0.332065\n",
      "epoch 77/500   error=0.199165\n",
      "epoch 78/500   error=0.153155\n",
      "epoch 79/500   error=0.140765\n",
      "epoch 80/500   error=0.353190\n",
      "epoch 81/500   error=0.169625\n",
      "epoch 82/500   error=0.160830\n",
      "epoch 83/500   error=0.152943\n",
      "epoch 84/500   error=0.145083\n",
      "epoch 85/500   error=0.137295\n",
      "epoch 86/500   error=0.129641\n",
      "epoch 87/500   error=0.122150\n",
      "epoch 88/500   error=0.114837\n",
      "epoch 89/500   error=0.290084\n",
      "epoch 90/500   error=0.111184\n",
      "epoch 91/500   error=0.099749\n",
      "epoch 92/500   error=0.091705\n",
      "epoch 93/500   error=0.084759\n",
      "epoch 94/500   error=0.078457\n",
      "epoch 95/500   error=0.072626\n",
      "epoch 96/500   error=0.067207\n",
      "epoch 97/500   error=0.053312\n",
      "epoch 98/500   error=0.275480\n",
      "epoch 99/500   error=0.263423\n",
      "epoch 100/500   error=0.060156\n",
      "epoch 101/500   error=0.053342\n",
      "epoch 102/500   error=0.048340\n",
      "epoch 103/500   error=0.044362\n",
      "epoch 104/500   error=0.041014\n",
      "epoch 105/500   error=0.033973\n",
      "epoch 106/500   error=0.265746\n",
      "epoch 107/500   error=0.260682\n",
      "epoch 108/500   error=0.034102\n",
      "epoch 109/500   error=0.259880\n",
      "epoch 110/500   error=0.033080\n",
      "epoch 111/500   error=0.027180\n",
      "epoch 112/500   error=0.031546\n",
      "epoch 113/500   error=0.026167\n",
      "epoch 114/500   error=0.025097\n",
      "epoch 115/500   error=0.023919\n",
      "epoch 116/500   error=0.022677\n",
      "epoch 117/500   error=0.021519\n",
      "epoch 118/500   error=0.020462\n",
      "epoch 119/500   error=0.261780\n",
      "epoch 120/500   error=0.018712\n",
      "epoch 121/500   error=0.017929\n",
      "epoch 122/500   error=0.017286\n",
      "epoch 123/500   error=0.016579\n",
      "epoch 124/500   error=0.015912\n",
      "epoch 125/500   error=0.256527\n",
      "epoch 126/500   error=0.014954\n",
      "epoch 127/500   error=0.014410\n",
      "epoch 128/500   error=0.255556\n",
      "epoch 129/500   error=0.258669\n",
      "epoch 130/500   error=0.013298\n",
      "epoch 131/500   error=0.012522\n",
      "epoch 132/500   error=0.011866\n",
      "epoch 133/500   error=0.013238\n",
      "epoch 134/500   error=0.011892\n",
      "epoch 135/500   error=0.011375\n",
      "epoch 136/500   error=0.011049\n",
      "epoch 137/500   error=0.010719\n",
      "epoch 138/500   error=0.010401\n",
      "epoch 139/500   error=0.010102\n",
      "epoch 140/500   error=0.009817\n",
      "epoch 141/500   error=0.009251\n",
      "epoch 142/500   error=0.009392\n",
      "epoch 143/500   error=0.009471\n",
      "epoch 144/500   error=0.008711\n",
      "epoch 145/500   error=0.253708\n",
      "epoch 146/500   error=0.007967\n",
      "epoch 147/500   error=0.009013\n",
      "epoch 148/500   error=0.008090\n",
      "epoch 149/500   error=0.500094\n",
      "epoch 150/500   error=0.007861\n",
      "epoch 151/500   error=0.007839\n",
      "epoch 152/500   error=0.007583\n",
      "epoch 153/500   error=0.007404\n",
      "epoch 154/500   error=0.007245\n",
      "epoch 155/500   error=0.007085\n",
      "epoch 156/500   error=0.006727\n",
      "epoch 157/500   error=0.006930\n",
      "epoch 158/500   error=0.006800\n",
      "epoch 159/500   error=0.006618\n",
      "epoch 160/500   error=0.006369\n",
      "epoch 161/500   error=0.006100\n",
      "epoch 162/500   error=0.253586\n",
      "epoch 163/500   error=0.253410\n",
      "epoch 164/500   error=0.005929\n",
      "epoch 165/500   error=0.006017\n",
      "epoch 166/500   error=0.252719\n",
      "epoch 167/500   error=0.252503\n",
      "epoch 168/500   error=0.005673\n",
      "epoch 169/500   error=0.005586\n",
      "epoch 170/500   error=0.005519\n",
      "epoch 171/500   error=0.005406\n",
      "epoch 172/500   error=0.005313\n",
      "epoch 173/500   error=0.005224\n",
      "epoch 174/500   error=0.005136\n",
      "epoch 175/500   error=0.005051\n",
      "epoch 176/500   error=0.252794\n",
      "epoch 177/500   error=0.004919\n",
      "epoch 178/500   error=0.252165\n",
      "epoch 179/500   error=0.004812\n",
      "epoch 180/500   error=0.004713\n",
      "epoch 181/500   error=0.252040\n",
      "epoch 182/500   error=0.004638\n",
      "epoch 183/500   error=0.004546\n",
      "epoch 184/500   error=0.252597\n",
      "epoch 185/500   error=0.004412\n",
      "epoch 186/500   error=0.004311\n",
      "epoch 187/500   error=0.252318\n",
      "epoch 188/500   error=0.004556\n",
      "epoch 189/500   error=0.251901\n",
      "epoch 190/500   error=0.251817\n",
      "epoch 191/500   error=0.004169\n",
      "epoch 192/500   error=0.252311\n",
      "epoch 193/500   error=0.004132\n",
      "epoch 194/500   error=0.251763\n",
      "epoch 195/500   error=0.252227\n",
      "epoch 196/500   error=0.004072\n",
      "epoch 197/500   error=0.003902\n",
      "epoch 198/500   error=0.003868\n",
      "epoch 199/500   error=0.251751\n",
      "epoch 200/500   error=0.003803\n",
      "epoch 201/500   error=0.003762\n",
      "epoch 202/500   error=0.003722\n",
      "epoch 203/500   error=0.003657\n",
      "epoch 204/500   error=0.500043\n",
      "epoch 205/500   error=0.003639\n",
      "epoch 206/500   error=0.251540\n",
      "epoch 207/500   error=0.003568\n",
      "epoch 208/500   error=0.003419\n",
      "epoch 209/500   error=0.003630\n",
      "epoch 210/500   error=0.003419\n",
      "epoch 211/500   error=0.251965\n",
      "epoch 212/500   error=0.003372\n",
      "epoch 213/500   error=0.003311\n",
      "epoch 214/500   error=0.251459\n",
      "epoch 215/500   error=0.003276\n",
      "epoch 216/500   error=0.003223\n",
      "epoch 217/500   error=0.003210\n",
      "epoch 218/500   error=0.003162\n",
      "epoch 219/500   error=0.003127\n",
      "epoch 220/500   error=0.251341\n",
      "epoch 221/500   error=0.251288\n",
      "epoch 222/500   error=0.003065\n",
      "epoch 223/500   error=0.003035\n",
      "epoch 224/500   error=0.002973\n",
      "epoch 225/500   error=0.002924\n",
      "epoch 226/500   error=0.003074\n",
      "epoch 227/500   error=0.002913\n",
      "epoch 228/500   error=0.002873\n",
      "epoch 229/500   error=0.002857\n",
      "epoch 230/500   error=0.002797\n",
      "epoch 231/500   error=0.002827\n",
      "epoch 232/500   error=0.002768\n",
      "epoch 233/500   error=0.002698\n",
      "epoch 234/500   error=0.002789\n",
      "epoch 235/500   error=0.002676\n",
      "epoch 236/500   error=0.002682\n",
      "epoch 237/500   error=0.251489\n",
      "epoch 238/500   error=0.002615\n",
      "epoch 239/500   error=0.002586\n",
      "epoch 240/500   error=0.002574\n",
      "epoch 241/500   error=0.002543\n",
      "epoch 242/500   error=0.251396\n",
      "epoch 243/500   error=0.251359\n",
      "epoch 244/500   error=0.002490\n",
      "epoch 245/500   error=0.002469\n",
      "epoch 246/500   error=0.002455\n",
      "epoch 247/500   error=0.002427\n",
      "epoch 248/500   error=0.002407\n",
      "epoch 249/500   error=0.002386\n",
      "epoch 250/500   error=0.002347\n",
      "epoch 251/500   error=0.002370\n",
      "epoch 252/500   error=0.002326\n",
      "epoch 253/500   error=0.002301\n",
      "epoch 254/500   error=0.251053\n",
      "epoch 255/500   error=0.002279\n",
      "epoch 256/500   error=0.002250\n",
      "epoch 257/500   error=0.251231\n",
      "epoch 258/500   error=0.002231\n",
      "epoch 259/500   error=0.002204\n",
      "epoch 260/500   error=0.002180\n",
      "epoch 261/500   error=0.500023\n",
      "epoch 262/500   error=0.002187\n",
      "epoch 263/500   error=0.002150\n",
      "epoch 264/500   error=0.500036\n",
      "epoch 265/500   error=0.002160\n",
      "epoch 266/500   error=0.002115\n",
      "epoch 267/500   error=0.002122\n",
      "epoch 268/500   error=0.002062\n",
      "epoch 269/500   error=0.002127\n",
      "epoch 270/500   error=0.002055\n",
      "epoch 271/500   error=0.002064\n",
      "epoch 272/500   error=0.002027\n",
      "epoch 273/500   error=0.002000\n",
      "epoch 274/500   error=0.001977\n",
      "epoch 275/500   error=0.002030\n",
      "epoch 276/500   error=0.001960\n",
      "epoch 277/500   error=0.001962\n",
      "epoch 278/500   error=0.001943\n",
      "epoch 279/500   error=0.001914\n",
      "epoch 280/500   error=0.001922\n",
      "epoch 281/500   error=0.001897\n",
      "epoch 282/500   error=0.500012\n",
      "epoch 283/500   error=0.001894\n",
      "epoch 284/500   error=0.001863\n",
      "epoch 285/500   error=0.001868\n",
      "epoch 286/500   error=0.001821\n",
      "epoch 287/500   error=0.001873\n",
      "epoch 288/500   error=0.001816\n",
      "epoch 289/500   error=0.001824\n",
      "epoch 290/500   error=0.001785\n",
      "epoch 291/500   error=0.250979\n",
      "epoch 292/500   error=0.001781\n",
      "epoch 293/500   error=0.001757\n",
      "epoch 294/500   error=0.001747\n",
      "epoch 295/500   error=0.001766\n",
      "epoch 296/500   error=0.001723\n",
      "epoch 297/500   error=0.001730\n",
      "epoch 298/500   error=0.001669\n",
      "epoch 299/500   error=0.001755\n",
      "epoch 300/500   error=0.500017\n",
      "epoch 301/500   error=0.001695\n",
      "epoch 302/500   error=0.001667\n",
      "epoch 303/500   error=0.500018\n",
      "epoch 304/500   error=0.001675\n",
      "epoch 305/500   error=0.001636\n",
      "epoch 306/500   error=0.250909\n",
      "epoch 307/500   error=0.001640\n",
      "epoch 308/500   error=0.001630\n",
      "epoch 309/500   error=0.001620\n",
      "epoch 310/500   error=0.001602\n",
      "epoch 311/500   error=0.001596\n",
      "epoch 312/500   error=0.250842\n",
      "epoch 313/500   error=0.001582\n",
      "epoch 314/500   error=0.001557\n",
      "epoch 315/500   error=0.001591\n",
      "epoch 316/500   error=0.250814\n",
      "epoch 317/500   error=0.001538\n",
      "epoch 318/500   error=0.001562\n",
      "epoch 319/500   error=0.001532\n",
      "epoch 320/500   error=0.250720\n",
      "epoch 321/500   error=0.001519\n",
      "epoch 322/500   error=0.001503\n",
      "epoch 323/500   error=0.001500\n",
      "epoch 324/500   error=0.001488\n",
      "epoch 325/500   error=0.001479\n",
      "epoch 326/500   error=0.001472\n",
      "epoch 327/500   error=0.001463\n",
      "epoch 328/500   error=0.250681\n",
      "epoch 329/500   error=0.250776\n",
      "epoch 330/500   error=0.001444\n",
      "epoch 331/500   error=0.001438\n",
      "epoch 332/500   error=0.001432\n",
      "epoch 333/500   error=0.001421\n",
      "epoch 334/500   error=0.001414\n",
      "epoch 335/500   error=0.250656\n",
      "epoch 336/500   error=0.250643\n",
      "epoch 337/500   error=0.001397\n",
      "epoch 338/500   error=0.001391\n",
      "epoch 339/500   error=0.250632\n",
      "epoch 340/500   error=0.250613\n",
      "epoch 341/500   error=0.001390\n",
      "epoch 342/500   error=0.001359\n",
      "epoch 343/500   error=0.250627\n",
      "epoch 344/500   error=0.001368\n",
      "epoch 345/500   error=0.001357\n",
      "epoch 346/500   error=0.001351\n",
      "epoch 347/500   error=0.001334\n",
      "epoch 348/500   error=0.001335\n",
      "epoch 349/500   error=0.001325\n",
      "epoch 350/500   error=0.001316\n",
      "epoch 351/500   error=0.250730\n",
      "epoch 352/500   error=0.001307\n",
      "epoch 353/500   error=0.001295\n",
      "epoch 354/500   error=0.250585\n",
      "epoch 355/500   error=0.001293\n",
      "epoch 356/500   error=0.001280\n",
      "epoch 357/500   error=0.001282\n",
      "epoch 358/500   error=0.001269\n",
      "epoch 359/500   error=0.001263\n",
      "epoch 360/500   error=0.001258\n",
      "epoch 361/500   error=0.001251\n",
      "epoch 362/500   error=0.001240\n",
      "epoch 363/500   error=0.001247\n",
      "epoch 364/500   error=0.001233\n",
      "epoch 365/500   error=0.001225\n",
      "epoch 366/500   error=0.001222\n",
      "epoch 367/500   error=0.250670\n",
      "epoch 368/500   error=0.001212\n",
      "epoch 369/500   error=0.001203\n",
      "epoch 370/500   error=0.250658\n",
      "epoch 371/500   error=0.001197\n",
      "epoch 372/500   error=0.001187\n",
      "epoch 373/500   error=0.001189\n",
      "epoch 374/500   error=0.001178\n",
      "epoch 375/500   error=0.001173\n",
      "epoch 376/500   error=0.001168\n",
      "epoch 377/500   error=0.250628\n",
      "epoch 378/500   error=0.001160\n",
      "epoch 379/500   error=0.250532\n",
      "epoch 380/500   error=0.001154\n",
      "epoch 381/500   error=0.001144\n",
      "epoch 382/500   error=0.001144\n",
      "epoch 383/500   error=0.001135\n",
      "epoch 384/500   error=0.250611\n",
      "epoch 385/500   error=0.001129\n",
      "epoch 386/500   error=0.001120\n",
      "epoch 387/500   error=0.001120\n",
      "epoch 388/500   error=0.001112\n",
      "epoch 389/500   error=0.001107\n",
      "epoch 390/500   error=0.001103\n",
      "epoch 391/500   error=0.001097\n",
      "epoch 392/500   error=0.001093\n",
      "epoch 393/500   error=0.001088\n",
      "epoch 394/500   error=0.500005\n",
      "epoch 395/500   error=0.250501\n",
      "epoch 396/500   error=0.001079\n",
      "epoch 397/500   error=0.001071\n",
      "epoch 398/500   error=0.001082\n",
      "epoch 399/500   error=0.001067\n",
      "epoch 400/500   error=0.001061\n",
      "epoch 401/500   error=0.001059\n",
      "epoch 402/500   error=0.001050\n",
      "epoch 403/500   error=0.001053\n",
      "epoch 404/500   error=0.250561\n",
      "epoch 405/500   error=0.250562\n",
      "epoch 406/500   error=0.001040\n",
      "epoch 407/500   error=0.250483\n",
      "epoch 408/500   error=0.001032\n",
      "epoch 409/500   error=0.250483\n",
      "epoch 410/500   error=0.001015\n",
      "epoch 411/500   error=0.001046\n",
      "epoch 412/500   error=0.001021\n",
      "epoch 413/500   error=0.001031\n",
      "epoch 414/500   error=0.001009\n",
      "epoch 415/500   error=0.001006\n",
      "epoch 416/500   error=0.001004\n",
      "epoch 417/500   error=0.000996\n",
      "epoch 418/500   error=0.000994\n",
      "epoch 419/500   error=0.250450\n",
      "epoch 420/500   error=0.000990\n",
      "epoch 421/500   error=0.000976\n",
      "epoch 422/500   error=0.000994\n",
      "epoch 423/500   error=0.000968\n",
      "epoch 424/500   error=0.250549\n",
      "epoch 425/500   error=0.000977\n",
      "epoch 426/500   error=0.000971\n",
      "epoch 427/500   error=0.000959\n",
      "epoch 428/500   error=0.000953\n",
      "epoch 429/500   error=0.250458\n",
      "epoch 430/500   error=0.000958\n",
      "epoch 431/500   error=0.000947\n",
      "epoch 432/500   error=0.500010\n",
      "epoch 433/500   error=0.000952\n",
      "epoch 434/500   error=0.250512\n",
      "epoch 435/500   error=0.000946\n",
      "epoch 436/500   error=0.000934\n",
      "epoch 437/500   error=0.000934\n",
      "epoch 438/500   error=0.000929\n",
      "epoch 439/500   error=0.000924\n",
      "epoch 440/500   error=0.000922\n",
      "epoch 441/500   error=0.000915\n",
      "epoch 442/500   error=0.000918\n",
      "epoch 443/500   error=0.000916\n",
      "epoch 444/500   error=0.000906\n",
      "epoch 445/500   error=0.250420\n",
      "epoch 446/500   error=0.250407\n",
      "epoch 447/500   error=0.000910\n",
      "epoch 448/500   error=0.000899\n",
      "epoch 449/500   error=0.250486\n",
      "epoch 450/500   error=0.000902\n",
      "epoch 451/500   error=0.000881\n",
      "epoch 452/500   error=0.000901\n",
      "epoch 453/500   error=0.250481\n",
      "epoch 454/500   error=0.000890\n",
      "epoch 455/500   error=0.000877\n",
      "epoch 456/500   error=0.250472\n",
      "epoch 457/500   error=0.000875\n",
      "epoch 458/500   error=0.000868\n",
      "epoch 459/500   error=0.000869\n",
      "epoch 460/500   error=0.000863\n",
      "epoch 461/500   error=0.000860\n",
      "epoch 462/500   error=0.250459\n",
      "epoch 463/500   error=0.000856\n",
      "epoch 464/500   error=0.000851\n",
      "epoch 465/500   error=0.250454\n",
      "epoch 466/500   error=0.000849\n",
      "epoch 467/500   error=0.000844\n",
      "epoch 468/500   error=0.250447\n",
      "epoch 469/500   error=0.000842\n",
      "epoch 470/500   error=0.000836\n",
      "epoch 471/500   error=0.000837\n",
      "epoch 472/500   error=0.250393\n",
      "epoch 473/500   error=0.000830\n",
      "epoch 474/500   error=0.000827\n",
      "epoch 475/500   error=0.000830\n",
      "epoch 476/500   error=0.000820\n",
      "epoch 477/500   error=0.000823\n",
      "epoch 478/500   error=0.000816\n",
      "epoch 479/500   error=0.250381\n",
      "epoch 480/500   error=0.250431\n",
      "epoch 481/500   error=0.000815\n",
      "epoch 482/500   error=0.000806\n",
      "epoch 483/500   error=0.000810\n",
      "epoch 484/500   error=0.000802\n",
      "epoch 485/500   error=0.000799\n",
      "epoch 486/500   error=0.250423\n",
      "epoch 487/500   error=0.000796\n",
      "epoch 488/500   error=0.000792\n",
      "epoch 489/500   error=0.250375\n",
      "epoch 490/500   error=0.250418\n",
      "epoch 491/500   error=0.000787\n",
      "epoch 492/500   error=0.000785\n",
      "epoch 493/500   error=0.000780\n",
      "epoch 494/500   error=0.000788\n",
      "epoch 495/500   error=0.000777\n",
      "epoch 496/500   error=0.000781\n",
      "epoch 497/500   error=0.000772\n",
      "epoch 498/500   error=0.250405\n",
      "epoch 499/500   error=0.250363\n",
      "epoch 500/500   error=0.000770\n",
      "[array([[0.00096374]]), array([[0.962176]]), array([[0.9599408]]), array([[-0.00136491]])]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "np.random.seed(10)\n",
    "net = Network()\n",
    "net.add(FCLayer(2, 10))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(10, 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(DropoutLayer(p=0.1))\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=500, learning_rate=0.1)\n",
    "out = net.predict(x_train)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the MNIST problem with TANH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/20   error=0.042247\n",
      "epoch 2/20   error=0.021373\n",
      "epoch 3/20   error=0.017366\n",
      "epoch 4/20   error=0.015063\n",
      "epoch 5/20   error=0.013448\n",
      "epoch 6/20   error=0.012274\n",
      "epoch 7/20   error=0.011323\n",
      "epoch 8/20   error=0.010582\n",
      "epoch 9/20   error=0.009878\n",
      "epoch 10/20   error=0.009291\n",
      "epoch 11/20   error=0.008769\n",
      "epoch 12/20   error=0.008260\n",
      "epoch 13/20   error=0.007880\n",
      "epoch 14/20   error=0.007516\n",
      "epoch 15/20   error=0.007213\n",
      "epoch 16/20   error=0.006890\n",
      "epoch 17/20   error=0.006653\n",
      "epoch 18/20   error=0.006448\n",
      "epoch 19/20   error=0.006197\n",
      "epoch 20/20   error=0.006017\n",
      "\n",
      "\n",
      "predicted values : \n",
      "[array([[ 4.54926533e-03, -6.59751279e-04, -1.80151899e-02,\n",
      "         2.71950495e-03,  4.32174363e-02, -6.55587823e-03,\n",
      "         7.91355701e-04,  9.89372049e-01, -1.38102060e-02,\n",
      "         1.13200466e-02]]), array([[0.0262188 , 0.00856112, 0.96242119, 0.03664727, 0.0156369 ,\n",
      "        0.04104411, 0.00994825, 0.01766055, 0.06400523, 0.02315558]]), array([[ 0.00277871,  0.993914  , -0.00762733,  0.0013321 , -0.01244486,\n",
      "         0.02320877,  0.00152822, -0.00342633, -0.00285468, -0.00537598]]), array([[ 0.99373009,  0.00566916, -0.00896768,  0.0102995 , -0.00475392,\n",
      "        -0.00234815,  0.00307021,  0.00363547, -0.00739924, -0.00265427]]), array([[-2.54785039e-04,  4.69734548e-03, -3.79117172e-03,\n",
      "         4.75661059e-03,  9.80575501e-01, -1.71553302e-02,\n",
      "         2.75199198e-03, -7.46781566e-03, -2.08726841e-03,\n",
      "         9.45748194e-03]]), array([[ 8.88810673e-04,  9.93915401e-01, -1.04266612e-02,\n",
      "         2.11942359e-03, -4.09772267e-03,  1.27561238e-02,\n",
      "         1.60964974e-03, -2.63147547e-04, -1.43529855e-03,\n",
      "        -4.25096038e-03]]), array([[ 0.00735376,  0.00400609, -0.01760309,  0.00795462,  0.98063183,\n",
      "        -0.01210399,  0.00242862, -0.00558223, -0.00110179,  0.01053942]]), array([[0.02967592, 0.01434814, 0.04806189, 0.03456643, 0.03004872,\n",
      "        0.06284669, 0.01275523, 0.01030798, 0.09182139, 0.67300281]]), array([[0.05927501, 0.01905999, 0.0499894 , 0.06001423, 0.0479462 ,\n",
      "        0.32710066, 0.01925346, 0.0265368 , 0.16332606, 0.07376243]]), array([[ 0.01322401,  0.0034416 ,  0.00259775,  0.00396364, -0.00555416,\n",
      "        -0.01529371,  0.00212561, -0.01155738, -0.01442472,  0.94034145]])]\n",
      "true values : \n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "y_train = to_categorical(y_train)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = to_categorical(y_test)\n",
    "np.random.seed(10)\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=20, learning_rate=0.1)\n",
    "out = net.predict(x_test[0:10])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the MNIST problem with RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/20   error=0.083660\n",
      "epoch 2/20   error=0.074360\n",
      "epoch 3/20   error=0.073893\n",
      "epoch 4/20   error=0.073555\n",
      "epoch 5/20   error=0.073185\n",
      "epoch 6/20   error=0.068493\n",
      "epoch 7/20   error=0.066521\n",
      "epoch 8/20   error=0.065671\n",
      "epoch 9/20   error=0.065261\n",
      "epoch 10/20   error=0.065193\n",
      "epoch 11/20   error=0.062004\n",
      "epoch 12/20   error=0.057549\n",
      "epoch 13/20   error=0.054199\n",
      "epoch 14/20   error=0.048081\n",
      "epoch 15/20   error=0.042809\n",
      "epoch 16/20   error=0.041666\n",
      "epoch 17/20   error=0.035522\n",
      "epoch 18/20   error=0.033087\n",
      "epoch 19/20   error=0.031982\n",
      "epoch 20/20   error=0.026341\n",
      "\n",
      "\n",
      "predicted values : \n",
      "[array([[0.04130301, 0.00277958, 0.01408205, 0.02674866, 0.28836682,\n",
      "        0.0261455 , 0.00971734, 0.04419697, 0.04137309, 0.31029831]]), array([[0.04130301, 0.00277958, 0.01408205, 0.02674866, 0.28836682,\n",
      "        0.0261455 , 0.00971734, 0.04419697, 0.04137309, 0.31029831]]), array([[0.00951132, 0.98396637, 0.00489434, 0.00146406, 0.11323585,\n",
      "        0.00496971, 0.00325671, 0.01527928, 0.00674477, 0.12069123]]), array([[ 9.97606379e-01, -3.97972882e-04,  4.50583247e-03,\n",
      "         6.38838503e-03,  1.20437663e-02, -9.11490674e-04,\n",
      "         1.26897866e-03,  1.14786834e-03,  6.31857352e-03,\n",
      "         4.04541447e-02]]), array([[0.04130301, 0.00277958, 0.01408205, 0.02674866, 0.28836682,\n",
      "        0.0261455 , 0.00971734, 0.04419697, 0.04137309, 0.31029831]]), array([[8.42944215e-03, 9.86370569e-01, 4.58185980e-03, 6.03957649e-04,\n",
      "        1.07085852e-01, 4.24937368e-03, 3.03699217e-03, 1.42951172e-02,\n",
      "        5.56636979e-03, 1.13996909e-01]]), array([[0.04130301, 0.00277958, 0.01408205, 0.02674866, 0.28836682,\n",
      "        0.0261455 , 0.00971734, 0.04419697, 0.04137309, 0.31029831]]), array([[0.04130301, 0.00277958, 0.01408205, 0.02674866, 0.28836682,\n",
      "        0.0261455 , 0.00971734, 0.04419697, 0.04137309, 0.31029831]]), array([[0.02537659, 0.00139169, 0.00910073, 0.01771305, 0.18181369,\n",
      "        0.8461487 , 0.00622216, 0.02725241, 0.02600023, 0.20084692]]), array([[0.04130301, 0.00277958, 0.01408205, 0.02674866, 0.28836682,\n",
      "        0.0261455 , 0.00971734, 0.04419697, 0.04137309, 0.31029831]])]\n",
      "true values : \n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "y_train = to_categorical(y_train)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = to_categorical(y_test)\n",
    "np.random.seed(10)\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))\n",
    "net.add(ActivationLayer(relu, relu_prime))\n",
    "net.add(DropoutLayer(p=0.1))\n",
    "net.add(FCLayer(100, 50))\n",
    "net.add(ActivationLayer(relu, relu_prime))\n",
    "net.add(DropoutLayer(p=0.1))\n",
    "net.add(FCLayer(50, 10))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=20, learning_rate=0.1)\n",
    "out = net.predict(x_test[0:10])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparativa de Resultados\n",
    "\n",
    "##### Solve the XOR problem without Dropout Layer vs Solve the XOR problem with Dropout Layer\n",
    "\n",
    "| Sample | Without Dropout | With Dropout |\n",
    "|--------|-----------------|--------------|\n",
    "| 1      | 0.698956        | 0.698956     |\n",
    "| 2      | 0.474182        | 0.474182     |\n",
    "| 3      | 0.348177        | 0.348177     |\n",
    "| ...    | ...             | ...          |\n",
    "| 500    | 0.000646        | 0.000770     |\n",
    "\n",
    "##### Solve the XOR problem without Dropout Layer vs Solve the XOR problem with Dropout Layer\n",
    "\n",
    "| Epoch | Error with TANH | Error with RELU |\n",
    "|-------|------------------|-----------------|\n",
    "| 1     | 0.042247         | 0.083660        |\n",
    "| 2     | 0.021373         | 0.074360        |\n",
    "| 3     | 0.017366         | 0.073893        |\n",
    "| ...   | ...              | ...             |\n",
    "| 20    | 0.006017         | 0.026341        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicación de la Elección de ReLU y tanh\n",
    "\n",
    "##### ReLU (Rectified Linear Unit):\n",
    "Es una función de activación no lineal que se define como f(x) = max(0, x).\n",
    "Es popular en redes neuronales profundas porque reduce el problema del gradiente desvaneciente y permite un entrenamiento más rápido.\n",
    "En las capas ocultas, ayuda a aprender características más complejas y a mejorar la capacidad de representación de la red.\n",
    "\n",
    "##### tanh (Tangente Hiperbólica):\n",
    "Es una función de activación no lineal que se define como f(x) = (e^x - e^-x) / (e^x + e^-x).\n",
    "La salida de tanh está en el rango [-1, 1], lo cual puede ser útil para normalizar las salidas.\n",
    "En la capa de salida, tanh puede ser beneficiosa para asegurar que las predicciones estén en un rango limitado, lo cual puede ser útil en ciertas aplicaciones de clasificación.\n",
    "\n",
    "### Conclusiones sobre el Uso de Dropout, TANH y ReLU\n",
    "\n",
    "##### Dropout\n",
    "1. **Efectividad del Dropout**:\n",
    "   - En el problema XOR, el uso del Dropout no mostró una mejora significativa en la reducción del error final. De hecho, el error final fue ligeramente mayor con Dropout (0.000770) comparado con sin Dropout (0.000646).\n",
    "   - Esto puede deberse a que el problema XOR es relativamente simple y no se beneficia tanto de la regularización que ofrece Dropout.\n",
    "   \n",
    "2. **Regularización y Sobreajuste**:\n",
    "   - Dropout es más efectivo en problemas complejos o cuando se tiene una gran cantidad de datos y modelos profundos, ya que ayuda a prevenir el sobreajuste al desactivar aleatoriamente neuronas durante el entrenamiento.\n",
    "   - En el caso del problema XOR, la simplicidad del modelo y los datos no requieren tanta regularización.\n",
    "\n",
    "##### Función de Activación TANH\n",
    "1. **Convergencia Inicial Rápida**:\n",
    "   - En el problema MNIST, la función de activación TANH mostró una convergencia inicial más rápida en comparación con ReLU. El error inicial fue menor con TANH (0.042247) que con ReLU (0.083660).\n",
    "   - Esto sugiere que TANH puede ser beneficioso para iniciar el aprendizaje, ya que su salida está centrada en cero, lo que puede ayudar a una mejor propagación de gradientes al inicio del entrenamiento.\n",
    "\n",
    "2. **Error Final Más Bajo**:\n",
    "   - Después de 20 épocas, el error final con TANH (0.006017) fue significativamente menor que con ReLU (0.026341). Esto indica que TANH puede ofrecer un mejor ajuste final para este conjunto de datos específico.\n",
    "   - Sin embargo, TANH puede sufrir del problema de gradientes desvanecientes en redes muy profundas, lo que no fue un problema en esta configuración relativamente superficial.\n",
    "\n",
    "##### Función de Activación ReLU\n",
    "1. **Capacidad de Aprendizaje en Redes Profundas**:\n",
    "   - ReLU es conocida por ser más efectiva en redes neuronales profundas, ya que mitiga el problema de gradientes desvanecientes al permitir que solo las neuronas con valores positivos pasen gradientes significativos.\n",
    "   - Aunque el error inicial y final fueron mayores con ReLU en este caso, en configuraciones más complejas y profundas, ReLU podría mostrar una mejor performance.\n",
    "\n",
    "2. **Robustez y Eficiencia**:\n",
    "   - ReLU es computacionalmente eficiente y simple, ya que simplemente pasa valores positivos y bloquea los negativos.\n",
    "   - Aunque no tuvo el menor error en este experimento, sigue siendo una opción robusta para muchos problemas de aprendizaje profundo.\n",
    "\n",
    "##### Conclusión General\n",
    "- **Dropout**: Efectivo en modelos complejos para prevenir sobreajuste, pero no siempre necesario para problemas simples.\n",
    "- **TANH**: Puede ofrecer una mejor convergencia inicial y ajuste final en problemas menos complejos, pero puede sufrir de gradientes desvanecientes en redes profundas.\n",
    "- **ReLU**: Eficiente y robusto, especialmente útil en redes profundas y configuraciones más complejas, aunque puede requerir ajustes de hiperparámetros para obtener mejores resultados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
